{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini project 1\n",
    "## Identifying the girl next door - a study in natural langauge processing\n",
    "This notebook contains the code with which we have generated the results of our analysis. Code explanations follow in markdown throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "from random import shuffle\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#nltk.download('punkt') # Uncomment if package not downloaded\n",
    "#nltk.download('stopwords') #  Uncomment if package not downloaded\n",
    "#nltk.download('PorterStemmer') #  Uncomment if package not downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data loading\n",
    "Data is loaded as a pandas dataframe from the \"cleaned\" .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xetrev\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cleanerstill.csv', sep=\";\")\n",
    "#data.head() # Uncomment to inspect head of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data balancing\n",
    "Firstly, we want to filter out any row where sex,age,ethnicity,essay0 (about me) and essay4 (interests) is not given.\n",
    "This is done by creating a number of masks of the original data. We then proceed to check the sizes of the groups of males and females. It becomes apparant that he male group is considerably bigger than the female group. Hence it is reduced, after which the male and female group is concatenated and shuffled into df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xetrev\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "mask = (data['ethnicity'] != ' ') & (type(data['ethnicity']) != float) & (data['age'] != ' ') & (data['sex'] != ' ') & (data['essay0'] != ' ') & (data['essay4'] != ' ')\n",
    "# mask removes all rows where ethnicity, age and sex is not given. Also removes rows where ethnicity is NaN. This particular value is not present for age and sex, hence it is not masked out.\n",
    "data_new = data[mask]\n",
    "\n",
    "data_rc = data_new.filter(['age','sex','ethnicity','essay0','essay4'], axis=1)\n",
    "\n",
    "mask_male = (data_rc['sex'] == 'm') # creates mask of males where sex evaluates to True if == 'm'\n",
    "mask_female = (data_rc['sex'] == 'f') # creates mask of females where sex evaluates to True if == 'f'\n",
    "\n",
    "data_om = data_rc[mask_male] # only males for relevant columns\n",
    "data_of = data_rc[mask_female] # only females for relevant columns\n",
    "\n",
    "data_om_reduced = data_om.sample(frac=0.6665) # returns a random sample of data_om where parameter frac describes size of sample relative to original data\n",
    "\n",
    "df_tmp = pd.concat([data_of, data_om_reduced], ignore_index=True)\n",
    "\n",
    "df_final = df_tmp.sample(frac=1) # gives a random sample of df_tmp of size frac (currently 34685 obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Pickling \n",
    "df_final is splitted into test,dev and train data in a 10/10/80 ratio, then pickled and exported as binary files. The data set is quite big and we do a large number of calculations throughout the analysis. The pickle module helps us save intermediary results speeding up calculations. As we do not need to visually inspect the data any further, we save as a binary file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_final[:len(df_final)//10]\n",
    "dev_data = df_final[-len(df_final)//10:]\n",
    "train_data = df_final[len(df_final)//10:-len(df_final)//10]\n",
    "\n",
    "outfile = open(\"Data/test_data\", 'wb')\n",
    "pickle.dump(test_data, outfile)\n",
    "outfile.close()\n",
    "outfile = open(\"Data/dev_data\", 'wb')\n",
    "pickle.dump(dev_data, outfile)\n",
    "outfile.close()\n",
    "outfile = open(\"Data/train_data\", 'wb')\n",
    "pickle.dump(train_data, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Finding frequent n-grams\n",
    "Considering the huge number of n-grams our dataset we do not want to calculate n-gram frequencies every single time we run an experiment for a specific label. Hence, we use two functions to ease the process. The first is ngram_generator_wlabels(), the second is freq_ngrams() as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2.1.1 ngram_generator_wlabels\n",
    "ngram_generator_wlabels() gives us, for each essay, a tuple containing three dictionaries. For each of these dictionaries the given essay is its key, and its corresponding value is a tuple containing a list of all the ngrams in the essay and the essay's given labels. \n",
    "\n",
    "Calculating n-grams for approx 35,000 essays is a bit time consuming and this scripts pickles the results and dump them to a binary file. Hence, we only need to create the ngrams once for each data set, saving a substantial amount of time when running experiments.\n",
    "\n",
    "The function takes two arguments; the name of the input file e.g. test_data, dev_data or train data from 1.2.1, and a name for an output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generator_wlabels(data):\n",
    "    essay_list = ['essay0','essay4']\n",
    "    stop_words = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    infile = open(data, 'rb')\n",
    "    data_file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    essay_unigrams = {}\n",
    "    essay_bigrams = {}\n",
    "    essay_trigrams = {}\n",
    "    '''essay_unigrams['essay0'] will contain a list of all unigrams for each essay, along with a dictionary of all values for the classifiers\n",
    "    You access it by doing essay_unigrams['essay0'][i] where i is an index for a tuple of each essay in essay0 and a dictionary of classifier values'''\n",
    "    classifiers = [\"age\", \"ethnicity\", \"sex\"]\n",
    "    for es in essay_list:\n",
    "        all_bigrams = []\n",
    "        essays = [(idx, e) for idx, e in data_file[es].iteritems()]\n",
    "        unigrams_list = []\n",
    "        bigrams_list = []\n",
    "        trigrams_list = []\n",
    "        for (i, essay) in essays:\n",
    "            tmp = []\n",
    "            tmp_list = []\n",
    "            essay_bigram_list = []\n",
    "            essay_trigram_list = [] \n",
    "            classifier_dictionary = {}\n",
    "            for clas in classifiers:\n",
    "                classifier_dictionary[clas] = data_file[clas][i]\n",
    "            if type(essay) != float:\n",
    "                tmp.extend([w for w in essay.split()])\n",
    "                for w in tmp:\n",
    "                    splt = w.split(\"'\")\n",
    "                    for s in splt:\n",
    "                        if not s.isdigit():\n",
    "                            tmp_list.append(porter.stem(s))\n",
    "                for j in range(len(tmp_list)-1):\n",
    "                    essay_bigram_list.append(\" \".join((tmp_list[j],tmp_list[j+1])))\n",
    "                for k in range(len(tmp_list)-2):\n",
    "                    essay_trigram_list.append(\" \".join((tmp_list[k],tmp_list[k+1],tmp_list[k+2])))\n",
    "                unigrams_list.append((tmp_list, classifier_dictionary))\n",
    "                bigrams_list.append((essay_bigram_list, classifier_dictionary))\n",
    "                trigrams_list.append((essay_trigram_list, classifier_dictionary))\n",
    "        essay_unigrams[es] = unigrams_list\n",
    "        essay_bigrams[es] = bigrams_list\n",
    "        essay_trigrams[es] = trigrams_list\n",
    "    return (essay_unigrams, essay_bigrams, essay_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_generator_wlabels('Data/train_data') # write e.g. ('')\n",
    "outfile = open('train_ngrams', 'wb') # write e.g. open('train_ngrams', 'wb')\n",
    "pickle.dump(ngrams, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 freq_ngrams\n",
    "freq_ngrams() takes no arguments from the user. Instead, it takes the binary train_data file from 1.2.1 and for each essay, here essay0 and essay4, first creates lists of all stemmed words,bigrams and unigrams unless the given token is a stopword. These lists are the all_words, all_bigrams and all_trigrams in the code below. \n",
    "\n",
    "It then proceeds to iterate through each of these lists calculating the frequency of the given n-gram in the document. It dumps in total 9 files; for each essay (essay0 and essay4) the frequency distributions of uni- bi- and trigrams in that essay. Then three files for the distribution of the combined n-grams of both essays - e.g. a list of all frequency distributions of unigrams in essay0 and the frequency distributions of all unigrams in essay4 for a given user in the data set. \n",
    "\n",
    "This script needs only to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngrams(essay_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    infile = open(\"Data/train_data\", 'rb')\n",
    "    train_data = pickle.load(infile)\n",
    "    every_word = []\n",
    "    every_bigram = []\n",
    "    every_trigram = []\n",
    "    \n",
    "    for es in essay_list:\n",
    "        all_words = []\n",
    "        all_bigrams = []\n",
    "        all_trigrams = []\n",
    "        essays = [(idx, e) for idx, e in train_data[es].iteritems()]\n",
    "        for i, essay in essays:\n",
    "            tmp = []\n",
    "            tmp_list = []\n",
    "            if type(essay) != float:\n",
    "                tmp.extend([porter.stem(w) for w in essay.split() if not w in stop_words])\n",
    "                for w in tmp:\n",
    "                    splt = w.split(\"'\")\n",
    "                    for s in splt:\n",
    "                        if not s.isdigit() and s not in stop_words:\n",
    "                            tmp_list.append(s)\n",
    "                for j in range(len(tmp_list)-1):\n",
    "                    all_bigrams.append(\" \".join((tmp_list[j],tmp_list[j+1])))\n",
    "                for k in range(len(tmp_list)-2):\n",
    "                    all_trigrams.append(\" \".join((tmp_list[k],tmp_list[k+1],tmp_list[k+2])))\n",
    "                all_words.extend(tmp_list)\n",
    "                \n",
    "        freq_words = nltk.FreqDist(w for w in all_words)\n",
    "        freq_bigrams = nltk.FreqDist(w for w in all_bigrams)\n",
    "        freq_trigrams = nltk.FreqDist(w for w in all_trigrams)\n",
    "        \n",
    "        with open(f\"Data/{es}_freq_words\", 'wb') as file:\n",
    "            pickle.dump(freq_words, file)\n",
    "        with open(f\"Data/{es}_freq_bigrams\", 'wb') as file:\n",
    "            pickle.dump(freq_bigrams, file)\n",
    "        with open(f\"Data/{es}_freq_trigrams\", 'wb') as file:\n",
    "            pickle.dump(freq_trigrams, file)\n",
    "\n",
    "        every_word.extend(all_words)\n",
    "        every_bigram.extend(all_bigrams)\n",
    "        every_trigram.extend(all_trigrams)\n",
    "        \n",
    "    freq_all_words = nltk.FreqDist(w for w in every_word)\n",
    "    freq_all_bigrams = nltk.FreqDist(w for w in every_bigram)\n",
    "    freq_all_trigrams = nltk.FreqDist(w for w in every_trigram)\n",
    "\n",
    "    with open(\"Data/all_freq_words\", 'wb') as file:\n",
    "        pickle.dump(freq_all_words, file)\n",
    "    with open(\"Data/all_freq_bigrams\", 'wb') as file:\n",
    "        pickle.dump(freq_all_bigrams, file)\n",
    "    with open(\"Data/all_freq_trigrams\", 'wb') as file:\n",
    "        pickle.dump(freq_all_trigrams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ngrams(['essay0','essay4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the experiment\n",
    "Now that all data is created we can run the actual Na√Øve-Bayes classifier. For this purpose we use the nltk library implemented through the function bayes(). Bayes() utilises label_func in order to create the relevant feature sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.1.1 label_func()\n",
    "label_func() is used for the labels that are not binary i.e. age and ethnicity. \n",
    "For these labels we need to group the data into larger groups. \n",
    "\n",
    "The function takes two parameters, l, which is a class like sex, age or ethnicity,\n",
    "and cdl which is a dictionary as given in ngram_generator_wlabels() that contains \n",
    "the labels for a given essay.\n",
    "\n",
    "If you call bayes() with unigrams, essay and sex, label_func will return m or f\n",
    "given whether the author is male or female. \n",
    "\n",
    "If bayes() is called with age, label_func will look up the age of the author of a given\n",
    "essay and return u_30 if the age of the author is 30 or less and o_30 if age is over 30.\n",
    "\n",
    "Lastly, if bayes() is called with ethnicity, it will give the individual user the label white or non-white.\n",
    "\n",
    "## 3.1.2 document_features()\n",
    "document_features is utilised to create the feature sets. It is called when defining the variables train_features and dev_features.\n",
    "\n",
    "This means the function is given a list of each ngram in each essay with its corresponding labels.\n",
    "\n",
    "From this, it returns a dictionary with each of the 2000 most frequent n-grams for all essays in the category as keys and a boolean of whether that n-gram is in the actual essay it is reviewing at the moment.\n",
    "    \n",
    "\n",
    "\n",
    "## 3.1.3 bayes()\n",
    "Our function bayes() takes four arguments; an integer describing whether we want to analyse for uni, bi or trigrams, an essay to analyse, a label to identify and a boolean to decide whether to run for most frequent n-grams in just the given essay or for both essays per user.\n",
    "\n",
    "An essential part of this function is creating the actual feature sets. It does so with for instance train_features by creating a list containing a tuple with two indexes. The first index contains a dictionary as created by document_features that is, for the 2000 most frequent n-grams for a given essay category, is each word in the specific essay or not, and for the second essay the label for the author of the specific essay. The label is returned by label_func().\n",
    "\n",
    "The train and dev sets are then shuffled with the random module, after which we create our classifier by training the NaiveBayesClassifier on the train set. \n",
    "\n",
    "From here we can run the classifier on the test data (in this case the test data is the dev data) computing the accuracy of the classifier and showing the 100 most informative features.\n",
    "\n",
    "We then proceed to create two sets, predictions and gold_labels that contain collections of the labels of each person in the data. We iterate through the feature set of the test data, and if the classifiers predicts e.g. male it adds the number of this observation to the collection of males. Same goes for the opposite label. gold_labels is a similar data structure containing the true label for each observation.\n",
    "\n",
    "We can then call nltk precision, recall and f-measure methods with predictions and gold labels in order to return the precision, recall and F1-score of the model.\n",
    "## 4. Further analysis\n",
    "The commented multiline section in bayes() after the calculation of precision,recall and F1-score contains contains other classification models which, if uncommented will return the accuracy of applying said model on our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(l, cdl):\n",
    "    if l == \"sex\" or cdl == \"white\":\n",
    "        return cdl\n",
    "    elif l == \"age\":\n",
    "        if cdl <= 30:\n",
    "            return \"u_30\"\n",
    "        else:\n",
    "            return \"o_30\"\n",
    "    else:\n",
    "        return \"n-white\"\n",
    "    \n",
    "def bayes(ngram, essay, label,all):\n",
    "    if ngram == 0:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_words\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_words\", 'rb')\n",
    "    elif ngram == 1:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_bigrams\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_bigrams\", 'rb')\n",
    "    else:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_trigrams\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_trigrams\", 'rb')\n",
    "            \n",
    "    freq_ngrams = pickle.load(infile)\n",
    "    word_features = [w for (w, f) in freq_ngrams.most_common(2000)]\n",
    "    infile.close()\n",
    "      \n",
    "    def document_features(document):\n",
    "        document_words = set(document)\n",
    "        features = {}\n",
    "        for word in word_features:\n",
    "            features['contains({})'.format(word)] = (word in document_words)\n",
    "        return features\n",
    "\n",
    "    infile = open(\"Data/train_ngrams\", 'rb')\n",
    "    train_ngrams = pickle.load(infile)\n",
    "    '''tuple with essay_unigrams, essay_bigrams and essay_trigrams\n",
    "    to access unigrams, just ngram_tuple[0][\"essay0\"]'''\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(\"Data/dev_ngrams\", 'rb')\n",
    "    dev_ngrams = pickle.load(infile)\n",
    "    infile.close()\n",
    "            \n",
    "    train_features = [(document_features(t), label_func(label, class_dic[label])) for (t, class_dic) in train_ngrams[ngram][essay]]\n",
    "    dev_features = [(document_features(t), label_func(label, class_dic[label])) for (t, class_dic) in dev_ngrams[ngram][essay]]\n",
    "    shuffle(train_features)\n",
    "    shuffle(dev_features)\n",
    "    training_set, testing_set = train_features, dev_features\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    print(\"Naive Bayes accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "    print(classifier.show_most_informative_features(100))\n",
    "    predictions, gold_labels = defaultdict(set), defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testing_set):\n",
    "        predictions[classifier.classify(features)].add(i)\n",
    "        gold_labels[label].add(i)\n",
    "    for label in predictions:\n",
    "        print(label, 'Precision:', precision(gold_labels[label], predictions[label]))\n",
    "        print(label, 'Recall:', recall(gold_labels[label], predictions[label]))\n",
    "        print(label, 'F1-Score:', f_measure(gold_labels[label], predictions[label]))\n",
    "        print()\n",
    "            \n",
    "    '''\n",
    "    MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "    MNB_classifier.train(training_set)\n",
    "    print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "    BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "    BernoulliNB_classifier.train(training_set)\n",
    "    print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "    LogisticRegression_classifier = SklearnClassifier(LogisticRegression(max_iter = 150))\n",
    "    LogisticRegression_classifier.train(training_set)\n",
    "    print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "    SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "    SGDClassifier_classifier.train(training_set)\n",
    "    print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "    LinearSVC_classifier = SklearnClassifier(LinearSVC(max_iter=1200))\n",
    "    LinearSVC_classifier.train(training_set)\n",
    "    print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy percent: 68.51100811123986\n",
      "Most Informative Features\n",
      "        contains(austen) = True                f : m      =      7.5 : 1.0\n",
      "      contains(prejudic) = True                f : m      =      6.7 : 1.0\n",
      "           contains(eyr) = True                f : m      =      5.2 : 1.0\n",
      "         contains(pride) = True                f : m      =      5.0 : 1.0\n",
      "       contains(anatomi) = True                f : m      =      4.7 : 1.0\n",
      "          contains(pray) = True                f : m      =      4.5 : 1.0\n",
      "       contains(barbara) = True                f : m      =      4.1 : 1.0\n",
      "        contains(geisha) = True                f : m      =      4.0 : 1.0\n",
      "          contains(etta) = True                f : m      =      4.0 : 1.0\n",
      "        contains(memoir) = True                f : m      =      3.8 : 1.0\n",
      "        contains(canada) = True                m : f      =      3.8 : 1.0\n",
      "          contains(jane) = True                f : m      =      3.7 : 1.0\n",
      "      contains(difranco) = True                f : m      =      3.7 : 1.0\n",
      "       contains(samurai) = True                m : f      =      3.6 : 1.0\n",
      "    contains(strangelov) = True                m : f      =      3.5 : 1.0\n",
      "     contains(middlesex) = True                f : m      =      3.5 : 1.0\n",
      "       contains(kubrick) = True                m : f      =      3.4 : 1.0\n",
      "        contains(gossip) = True                f : m      =      3.4 : 1.0\n",
      "          contains(gear) = True                m : f      =      3.4 : 1.0\n",
      "     contains(apocalyps) = True                m : f      =      3.3 : 1.0\n",
      "        contains(asimov) = True                m : f      =      3.3 : 1.0\n",
      "        contains(crimin) = True                f : m      =      3.3 : 1.0\n",
      "          contains(ador) = True                f : m      =      3.3 : 1.0\n",
      "          contains(grey) = True                f : m      =      3.3 : 1.0\n",
      "    contains(stephenson) = True                m : f      =      3.2 : 1.0\n",
      "    contains(bridesmaid) = True                f : m      =      3.2 : 1.0\n",
      "            contains(wu) = True                m : f      =      3.1 : 1.0\n",
      "       contains(downton) = True                f : m      =      3.1 : 1.0\n",
      "         contains(akira) = True                m : f      =      3.0 : 1.0\n",
      "           contains(ann) = True                f : m      =      3.0 : 1.0\n",
      "         contains(patti) = True                f : m      =      3.0 : 1.0\n",
      "        contains(farmer) = True                f : m      =      2.9 : 1.0\n",
      "           contains(svu) = True                f : m      =      2.8 : 1.0\n",
      "        contains(gibson) = True                m : f      =      2.8 : 1.0\n",
      "     contains(lamontagn) = True                f : m      =      2.8 : 1.0\n",
      "        contains(archer) = True                m : f      =      2.8 : 1.0\n",
      "          contains(tang) = True                m : f      =      2.8 : 1.0\n",
      "         contains(yummi) = True                f : m      =      2.7 : 1.0\n",
      "       contains(florenc) = True                f : m      =      2.7 : 1.0\n",
      "           contains(yum) = True                f : m      =      2.7 : 1.0\n",
      "         contains(chick) = True                f : m      =      2.7 : 1.0\n",
      "        contains(termin) = True                m : f      =      2.6 : 1.0\n",
      "        contains(ventur) = True                m : f      =      2.6 : 1.0\n",
      "       contains(holiday) = True                f : m      =      2.6 : 1.0\n",
      "      contains(futurama) = True                m : f      =      2.6 : 1.0\n",
      "           contains(zen) = True                m : f      =      2.5 : 1.0\n",
      "          contains(adel) = True                f : m      =      2.5 : 1.0\n",
      "            contains(na) = True                m : f      =      2.5 : 1.0\n",
      "       contains(pilgrim) = True                m : f      =      2.5 : 1.0\n",
      "      contains(notebook) = True                f : m      =      2.5 : 1.0\n",
      "        contains(butler) = True                f : m      =      2.5 : 1.0\n",
      "         contains(woman) = True                f : m      =      2.5 : 1.0\n",
      "        contains(brunch) = True                f : m      =      2.5 : 1.0\n",
      "     contains(metallica) = True                m : f      =      2.5 : 1.0\n",
      "          contains(anna) = True                f : m      =      2.5 : 1.0\n",
      "        contains(runway) = True                f : m      =      2.4 : 1.0\n",
      "       contains(chelsea) = True                f : m      =      2.4 : 1.0\n",
      "          contains(rent) = True                f : m      =      2.4 : 1.0\n",
      "        contains(knight) = True                m : f      =      2.4 : 1.0\n",
      "        contains(trashi) = True                f : m      =      2.4 : 1.0\n",
      "        contains(atwood) = True                f : m      =      2.4 : 1.0\n",
      "         contains(avett) = True                f : m      =      2.4 : 1.0\n",
      "         contains(salli) = True                f : m      =      2.4 : 1.0\n",
      "         contains(abbey) = True                f : m      =      2.4 : 1.0\n",
      "        contains(chocol) = True                f : m      =      2.4 : 1.0\n",
      "         contains(blade) = True                m : f      =      2.4 : 1.0\n",
      "      contains(watchmen) = True                m : f      =      2.4 : 1.0\n",
      "      contains(introduc) = True                f : m      =      2.4 : 1.0\n",
      "          contains(neal) = True                m : f      =      2.3 : 1.0\n",
      "         contains(wheel) = True                m : f      =      2.3 : 1.0\n",
      "          contains(heat) = True                m : f      =      2.3 : 1.0\n",
      "          contains(dick) = True                m : f      =      2.3 : 1.0\n",
      "        contains(housew) = True                f : m      =      2.3 : 1.0\n",
      "          contains(bone) = True                f : m      =      2.3 : 1.0\n",
      "        contains(yogurt) = True                f : m      =      2.3 : 1.0\n",
      "            contains(fm) = True                m : f      =      2.3 : 1.0\n",
      "      contains(mythbust) = True                m : f      =      2.3 : 1.0\n",
      "             contains(h) = True                m : f      =      2.3 : 1.0\n",
      "          contains(doom) = True                m : f      =      2.3 : 1.0\n",
      "        contains(matrix) = True                m : f      =      2.3 : 1.0\n",
      "        contains(height) = True                f : m      =      2.3 : 1.0\n",
      "          contains(wife) = True                f : m      =      2.3 : 1.0\n",
      "          contains(dune) = True                m : f      =      2.3 : 1.0\n",
      "         contains(fiona) = True                f : m      =      2.3 : 1.0\n",
      "      contains(winehous) = True                f : m      =      2.2 : 1.0\n",
      "         contains(salsa) = True                f : m      =      2.2 : 1.0\n",
      "          contains(kale) = True                f : m      =      2.2 : 1.0\n",
      "          contains(rage) = True                m : f      =      2.2 : 1.0\n",
      "      contains(entourag) = True                m : f      =      2.2 : 1.0\n",
      "            contains(dj) = True                m : f      =      2.2 : 1.0\n",
      "         contains(feist) = True                f : m      =      2.2 : 1.0\n",
      "          contains(hess) = True                m : f      =      2.2 : 1.0\n",
      "       contains(odyssey) = True                m : f      =      2.2 : 1.0\n",
      "       contains(simpson) = True                m : f      =      2.2 : 1.0\n",
      "           contains(bro) = True                m : f      =      2.2 : 1.0\n",
      "        contains(erykah) = True                f : m      =      2.2 : 1.0\n",
      "          contains(fave) = True                f : m      =      2.1 : 1.0\n",
      "         contains(loath) = True                m : f      =      2.1 : 1.0\n",
      "     contains(groundhog) = True                m : f      =      2.1 : 1.0\n",
      "     contains(embarrass) = True                f : m      =      2.1 : 1.0\n",
      "None\n",
      "m Precision: 0.678343949044586\n",
      "m Recall: 0.7265491756679932\n",
      "m F1-Score: 0.7016195443315948\n",
      "\n",
      "f Precision: 0.6932397959183674\n",
      "f Recall: 0.642055522740697\n",
      "f F1-Score: 0.6666666666666666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#bayes(ngram, essay, label,bool) # run with parameters e.g. as below\n",
    "all = False\n",
    "bayes(0, 'essay4', 'sex',False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
