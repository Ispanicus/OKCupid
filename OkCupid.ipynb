{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini project 1\n",
    "## Identifying the girl next door - a study in natural langauge processing\n",
    "This notebook contains the code with which we have generated the results of our analysis. Code explanations follow in markdown throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "from random import shuffle\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import collections\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.metrics.scores import precision, recall, f_measure\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "#nltk.download('punkt') # Uncomment if package not downloaded\n",
    "#nltk.download('stopwords') #  Uncomment if package not downloaded\n",
    "#nltk.download('PorterStemmer') #  Uncomment if package not downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data loading\n",
    "Data is loaded as a pandas dataframe from the \"cleaned\" .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xetrev\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cleanerstill.csv', sep=\";\")\n",
    "#data.head() # Uncomment to inspect head of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data balancing\n",
    "Firstly, we want to filter out any row where sex,age,ethnicity,essay0 (about me) and essay4 (interests) is not given.\n",
    "This is done by creating a number of masks of the original data. We then proceed to check the sizes of the groups of males and females. It becomes apparant that he male group is considerably bigger than the female group. Hence it is reduced, after which the male and female group is concatenated and shuffled into df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xetrev\\Anaconda3\\lib\\site-packages\\pandas\\core\\ops\\__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = method(y)\n"
     ]
    }
   ],
   "source": [
    "mask = (data['ethnicity'] != ' ') & (type(data['ethnicity']) != float) & (data['age'] != ' ') & (data['sex'] != ' ') & (data['essay0'] != ' ') & (data['essay4'] != ' ')\n",
    "# mask removes all rows where ethnicity, age and sex is not given. Also removes rows where ethnicity is NaN. This particular value is not present for age and sex, hence it is not masked out.\n",
    "data_new = data[mask]\n",
    "\n",
    "data_rc = data_new.filter(['age','sex','ethnicity','essay0','essay4'], axis=1)\n",
    "\n",
    "mask_male = (data_rc['sex'] == 'm') # creates mask of males where sex evaluates to True if == 'm'\n",
    "mask_female = (data_rc['sex'] == 'f') # creates mask of females where sex evaluates to True if == 'f'\n",
    "\n",
    "data_om = data_rc[mask_male] # only males for relevant columns\n",
    "data_of = data_rc[mask_female] # only females for relevant columns\n",
    "\n",
    "data_om_reduced = data_om.sample(frac=0.6665) # returns a random sample of data_om where parameter frac describes size of sample relative to original data\n",
    "\n",
    "df_tmp = pd.concat([data_of, data_om_reduced], ignore_index=True)\n",
    "\n",
    "df_final = df_tmp.sample(frac=1) # gives a random sample of df_tmp of size frac (currently 34685 obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Pickling \n",
    "df_final is splitted into test,dev and train data in a 10/10/80 ratio, then pickled and exported as binary files. The data set is quite big and we do a large number of calculations throughout the analysis. The pickle module helps us save intermediary results speeding up calculations. As we do not need to visually inspect the data any further, we save as a binary file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_final[:len(df_final)//10]\n",
    "dev_data = df_final[-len(df_final)//10:]\n",
    "train_data = df_final[len(df_final)//10:-len(df_final)//10]\n",
    "\n",
    "outfile = open(\"Data/test_data\", 'wb')\n",
    "pickle.dump(test_data, outfile)\n",
    "outfile.close()\n",
    "outfile = open(\"Data/dev_data\", 'wb')\n",
    "pickle.dump(dev_data, outfile)\n",
    "outfile.close()\n",
    "outfile = open(\"Data/train_data\", 'wb')\n",
    "pickle.dump(train_data, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Finding frequent n-grams\n",
    "Considering the huge number of n-grams our dataset we do not want to calculate n-gram frequencies every single time we run an experiment for a specific label. Hence, we use two functions to ease the process. The first is ngram_generator_wlabels(), the second is freq_ngrams() as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2.1.1 ngram_generator_wlabels\n",
    "ngram_generator_wlabels() gives us, for each essay, a tuple containing three dictionaries. For each of these dictionaries the given essay is its key, and its corresponding value is a tuple containing a list of all the ngrams in the essay and the essay's given labels. \n",
    "\n",
    "Calculating n-grams for approx 35,000 essays is a bit time consuming and this scripts pickles the results and dump them to a binary file. Hence, we only need to create the ngrams once for each data set, saving a substantial amount of time when running experiments.\n",
    "\n",
    "The function takes two arguments; the name of the input file e.g. test_data, dev_data or train data from 1.2.1, and a name for an output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generator_wlabels(data):\n",
    "    essay_list = ['essay0','essay4']\n",
    "    stop_words = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    infile = open(data, 'rb')\n",
    "    data_file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    essay_unigrams = {}\n",
    "    essay_bigrams = {}\n",
    "    essay_trigrams = {}\n",
    "    '''essay_unigrams['essay0'] will contain a list of all unigrams for each essay, along with a dictionary of all values for the classifiers\n",
    "    You access it by doing essay_unigrams['essay0'][i] where i is an index for a tuple of each essay in essay0 and a dictionary of classifier values'''\n",
    "    classifiers = [\"age\", \"ethnicity\", \"sex\"]\n",
    "    for es in essay_list:\n",
    "        all_bigrams = []\n",
    "        essays = [(idx, e) for idx, e in data_file[es].iteritems()]\n",
    "        unigrams_list = []\n",
    "        bigrams_list = []\n",
    "        trigrams_list = []\n",
    "        for (i, essay) in essays:\n",
    "            tmp = []\n",
    "            tmp_list = []\n",
    "            essay_bigram_list = []\n",
    "            essay_trigram_list = [] \n",
    "            classifier_dictionary = {}\n",
    "            for clas in classifiers:\n",
    "                classifier_dictionary[clas] = data_file[clas][i]\n",
    "            if type(essay) != float:\n",
    "                tmp.extend([w for w in essay.split()])\n",
    "                for w in tmp:\n",
    "                    splt = w.split(\"'\")\n",
    "                    for s in splt:\n",
    "                        if not s.isdigit():\n",
    "                            tmp_list.append(porter.stem(s))\n",
    "                for j in range(len(tmp_list)-1):\n",
    "                    essay_bigram_list.append(\" \".join((tmp_list[j],tmp_list[j+1])))\n",
    "                for k in range(len(tmp_list)-2):\n",
    "                    essay_trigram_list.append(\" \".join((tmp_list[k],tmp_list[k+1],tmp_list[k+2])))\n",
    "                unigrams_list.append((tmp_list, classifier_dictionary))\n",
    "                bigrams_list.append((essay_bigram_list, classifier_dictionary))\n",
    "                trigrams_list.append((essay_trigram_list, classifier_dictionary))\n",
    "        essay_unigrams[es] = unigrams_list\n",
    "        essay_bigrams[es] = bigrams_list\n",
    "        essay_trigrams[es] = trigrams_list\n",
    "    return (essay_unigrams, essay_bigrams, essay_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_generator_wlabels('Data/train_data') # write e.g. ('')\n",
    "outfile = open('train_ngrams', 'wb') # write e.g. open('train_ngrams', 'wb')\n",
    "pickle.dump(ngrams, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 freq_ngrams\n",
    "freq_ngrams() takes no arguments from the user. Instead, it takes the binary train_data file from 1.2.1 and for each essay, here essay0 and essay4, first creates lists of all stemmed words,bigrams and unigrams unless the given token is a stopword. These lists are the all_words, all_bigrams and all_trigrams in the code below. \n",
    "\n",
    "It then proceeds to iterate through each of these lists calculating the frequency of the given n-gram in the document. It dumps in total 9 files; for each essay (essay0 and essay4) the frequency distributions of uni- bi- and trigrams in that essay. Then three files for the distribution of the combined n-grams of both essays - e.g. a list of all frequency distributions of unigrams in essay0 and the frequency distributions of all unigrams in essay4 for a given user in the data set. \n",
    "\n",
    "This script needs only to be run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_ngrams(essay_list):\n",
    "    stop_words = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    infile = open(\"Data/train_data\", 'rb')\n",
    "    train_data = pickle.load(infile)\n",
    "    every_word = []\n",
    "    every_bigram = []\n",
    "    every_trigram = []\n",
    "    \n",
    "    for es in essay_list:\n",
    "        all_words = []\n",
    "        all_bigrams = []\n",
    "        all_trigrams = []\n",
    "        essays = [(idx, e) for idx, e in train_data[es].iteritems()]\n",
    "        for i, essay in essays:\n",
    "            tmp = []\n",
    "            tmp_list = []\n",
    "            if type(essay) != float:\n",
    "                tmp.extend([porter.stem(w) for w in essay.split() if not w in stop_words])\n",
    "                for w in tmp:\n",
    "                    splt = w.split(\"'\")\n",
    "                    for s in splt:\n",
    "                        if not s.isdigit() and s not in stop_words:\n",
    "                            tmp_list.append(s)\n",
    "                for j in range(len(tmp_list)-1):\n",
    "                    all_bigrams.append(\" \".join((tmp_list[j],tmp_list[j+1])))\n",
    "                for k in range(len(tmp_list)-2):\n",
    "                    all_trigrams.append(\" \".join((tmp_list[k],tmp_list[k+1],tmp_list[k+2])))\n",
    "                all_words.extend(tmp_list)\n",
    "                \n",
    "        freq_words = nltk.FreqDist(w for w in all_words)\n",
    "        freq_bigrams = nltk.FreqDist(w for w in all_bigrams)\n",
    "        freq_trigrams = nltk.FreqDist(w for w in all_trigrams)\n",
    "        \n",
    "        with open(f\"Data/{es}_freq_words\", 'wb') as file:\n",
    "            pickle.dump(freq_words, file)\n",
    "        with open(f\"Data/{es}_freq_bigrams\", 'wb') as file:\n",
    "            pickle.dump(freq_bigrams, file)\n",
    "        with open(f\"Data/{es}_freq_trigrams\", 'wb') as file:\n",
    "            pickle.dump(freq_trigrams, file)\n",
    "\n",
    "        every_word.extend(all_words)\n",
    "        every_bigram.extend(all_bigrams)\n",
    "        every_trigram.extend(all_trigrams)\n",
    "        \n",
    "    freq_all_words = nltk.FreqDist(w for w in every_word)\n",
    "    freq_all_bigrams = nltk.FreqDist(w for w in every_bigram)\n",
    "    freq_all_trigrams = nltk.FreqDist(w for w in every_trigram)\n",
    "\n",
    "    with open(\"Data/all_freq_words\", 'wb') as file:\n",
    "        pickle.dump(freq_all_words, file)\n",
    "    with open(\"Data/all_freq_bigrams\", 'wb') as file:\n",
    "        pickle.dump(freq_all_bigrams, file)\n",
    "    with open(\"Data/all_freq_trigrams\", 'wb') as file:\n",
    "        pickle.dump(freq_all_trigrams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_ngrams(['essay0','essay4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Running the experiment\n",
    "## 3.1 Bayes()\n",
    "Now that all data is created we can run the actual Na√Øve-Bayes classifier. For this purpose we use the nltk library implemented through the function bayes(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ngram = int(sys.argv[1])\n",
    "#essay = sys.argv[2]\n",
    "#label = sys.argv[3]\n",
    "#all = sys.argv[4]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_func(l, cdl):\n",
    "    if l == \"sex\" or cdl == \"white\":\n",
    "        return cdl\n",
    "    elif l == \"age\":\n",
    "        if cdl <= 30:\n",
    "            return \"u_30\"\n",
    "        else:\n",
    "            return \"o_30\"\n",
    "    else:\n",
    "        return \"n-white\"\n",
    "\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains({})'.format(word)] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "def bayes(ngram, essay, label,all):\n",
    "    if ngram == 0:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_words\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_words\", 'rb')\n",
    "    elif ngram == 1:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_bigrams\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_bigrams\", 'rb')\n",
    "    else:\n",
    "        if all:\n",
    "            infile = open(f\"Data/all_freq_trigrams\", 'rb')\n",
    "        else:\n",
    "            infile = open(f\"Data/{essay}_freq_trigrams\", 'rb')\n",
    "            \n",
    "    freq_ngrams = pickle.load(infile)\n",
    "    word_features = [w for (w, f) in freq_ngrams.most_common(2000)]\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(\"Data/train_ngrams\", 'rb')\n",
    "    train_ngrams = pickle.load(infile)\n",
    "    '''tuple with essay_unigrams, essay_bigrams and essay_trigrams\n",
    "    to access unigrams, just ngram_tuple[0][\"essay0\"]'''\n",
    "    infile.close()\n",
    "    \n",
    "    infile = open(\"Data/dev_ngrams\", 'rb')\n",
    "    dev_ngrams = pickle.load(infile)\n",
    "    infile.close()\n",
    "            \n",
    "    train_features = [(document_features(t), label_func(label, class_dic[label])) for (t, class_dic) in train_ngrams[ngram][essay]]\n",
    "    dev_features = [(document_features(t), label_func(label, class_dic[label])) for (t, class_dic) in dev_ngrams[ngram][essay]]\n",
    "    shuffle(train_features)\n",
    "    shuffle(dev_features)\n",
    "    training_set, testing_set = train_features, dev_features\n",
    "    classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "    print(\"Naive Bayes accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "    print(classifier.show_most_informative_features(100))\n",
    "    predictions, gold_labels = defaultdict(set), defaultdict(set)\n",
    "    for i, (features, label) in enumerate(testing_set):\n",
    "        predictions[classifier.classify(features)].add(i)\n",
    "        gold_labels[label].add(i)\n",
    "    for label in predictions:\n",
    "        print(label, 'Precision:', precision(gold_labels[label], predictions[label]))\n",
    "        print(label, 'Recall:', recall(gold_labels[label], predictions[label]))\n",
    "        print(label, 'F1-Score:', f_measure(gold_labels[label], predictions[label]))\n",
    "        print()\n",
    "    '''\n",
    "    MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "    MNB_classifier.train(training_set)\n",
    "    print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "    BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "    BernoulliNB_classifier.train(training_set)\n",
    "    print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "    LogisticRegression_classifier = SklearnClassifier(LogisticRegression(max_iter = 150))\n",
    "    LogisticRegression_classifier.train(training_set)\n",
    "    print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "    SGDClassifier_classifier = SklearnClassifier(SGDClassifier())\n",
    "    SGDClassifier_classifier.train(training_set)\n",
    "    print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGDClassifier_classifier, testing_set))*100)\n",
    "\n",
    "    LinearSVC_classifier = SklearnClassifier(LinearSVC(max_iter=1200))\n",
    "    LinearSVC_classifier.train(training_set)\n",
    "    print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)'''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 34202 samples and 1741467 outcomes>\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-51fb2a6e55bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#bayes(ngram, essay, label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbayes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'essay0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sex'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-d0dd83ef7561>\u001b[0m in \u001b[0;36mbayes\u001b[1;34m(ngram, essay, label, all)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mdev_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_ngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d0dd83ef7561>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0minfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mdev_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_dic\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdev_ngrams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mngram\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0messay\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-d0dd83ef7561>\u001b[0m in \u001b[0;36mdocument_features\u001b[1;34m(document)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdocument_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'contains({})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_features' is not defined"
     ]
    }
   ],
   "source": [
    "#bayes(ngram, essay, label)\n",
    "all = False\n",
    "print(bayes(0, 'essay0', 'sex',False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
