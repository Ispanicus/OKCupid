{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini project 1\n",
    "## Identifying the girl next door - a study in natural langauge processing\n",
    "This notebook contains the code with which we have generated the results of our analysis. Code explanations follow in markdown throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "import pickle\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from random import shuffle\n",
    "#nltk.download('punkt') # Uncomment if package not downloaded\n",
    "#nltk.download('stopwords') #  Uncomment if package not downloaded\n",
    "#nltk.download('PorterStemmer') #  Uncomment if package not downloaded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Data loading\n",
    "Data is loaded as a pandas dataframe from the \"cleaned\" .csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\xetrev\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cleanerstill.csv', sep=\";\")\n",
    "#data.head() # Uncomment to inspect head of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data balancing\n",
    "Firstly, we want to filter out any row where sex,age,ethnicity,essay0 (about me) and essay4 (interests) is not given.\n",
    "This is done by creating a number of masks of the original data. We then proceed to check the sizes of the groups of males and females. It becomes apparant that he male group is considerably bigger than the female group. Hence it is reduced, after which the male and female group is concatenated and shuffled into df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (data['ethnicity'] != ' ') & (type(data['ethnicity']) != float) & (data['age'] != ' ') & (data['sex'] != ' ') & (data['essay0'] != ' ') & (data['essay4'] != ' ')\n",
    "# mask removes all rows where ethnicity, age and sex is not given. Also removes rows where ethnicity is NaN. This particular value is not present for age and sex, hence it is not masked out.\n",
    "data_new = data[mask]\n",
    "\n",
    "data_rc = data_new.filter(['age','sex','ethnicity','essay0','essay4'], axis=1)\n",
    "\n",
    "mask_male = (data_rc['sex'] == 'm') # creates mask of males where sex evaluates to True if == 'm'\n",
    "mask_female = (data_rc['sex'] == 'f') # creates mask of females where sex evaluates to True if == 'f'\n",
    "\n",
    "data_om = data_rc[mask_male] # only males for relevant columns\n",
    "data_of = data_rc[mask_female] # only females for relevant columns\n",
    "\n",
    "data_om_reduced = data_om.sample(frac=0.6665) # returns a random sample of data_om where parameter frac describes size of sample relative to original data\n",
    "\n",
    "df_tmp = pd.concat([data_of, data_om_reduced], ignore_index=True)\n",
    "\n",
    "df_final = df_tmp.sample(frac=1) # gives a random sample of df_tmp of size frac (currently 34685 obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Pickling \n",
    "df_final is pickled and exported as a binary file. The data set is quite big and we do a large number of calculations throughout the analysis. The pickle module helps us save intermediary results speeding up calculations. As we do not need to visually inspect the data any further, we save as a binary file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = open(\"Data/cleaned_data\", 'wb')\n",
    "pickle.dump(df_final, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Finding frequent n-grams\n",
    "Considering the huge number of n-grams our dataset we do not want to calculate n-gram frequencies every single time we run an experiment for a specific label. Hence, we use two functions to ease the process. The first is ngram_generator_wlabels(), the second is freq_ngrams() as explained below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2.1.1 ngram_generator_wlabels\n",
    "ngram_generator_wlabels() gives us, for each essay, a tuple containing three dictionaries. For each of these dictionaries the given essay is its key, and its corresponding value is a tuple containing a list of all the ngrams in the essay and the essay's given labels. \n",
    "\n",
    "Calculating n-grams for approx 35,000 essays is a bit time consuming and this scripts pickles the results and dump them to a binary file. Hence, we only need to create the ngrams once, saving a substantial amount of time when running experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generator_wlabels(data):\n",
    "    essay_list = ['essay0','essay4']\n",
    "    stop_words = stopwords.words('english')\n",
    "    porter = PorterStemmer()\n",
    "    infile = open(data, 'rb')\n",
    "    data_file = pickle.load(infile)\n",
    "    infile.close()\n",
    "    essay_unigrams = {}\n",
    "    essay_bigrams = {}\n",
    "    essay_trigrams = {}\n",
    "    '''essay_unigrams['essay0'] will contain a list of all unigrams for each essay, along with a dictionary of all values for the classifiers\n",
    "    You access it by doing essay_unigrams['essay0'][i] where i is an index for a tuple of each essay in essay0 and a dictionary of classifier values'''\n",
    "    classifiers = [\"age\", \"ethnicity\", \"sex\"]\n",
    "    for es in essay_list:\n",
    "        all_bigrams = []\n",
    "        essays = [(idx, e) for idx, e in data_file[es].iteritems()]\n",
    "        unigrams_list = []\n",
    "        bigrams_list = []\n",
    "        trigrams_list = []\n",
    "        for (i, essay) in essays:\n",
    "            tmp = []\n",
    "            tmp_list = []\n",
    "            essay_bigram_list = []\n",
    "            essay_trigram_list = [] \n",
    "            classifier_dictionary = {}\n",
    "            for clas in classifiers:\n",
    "                classifier_dictionary[clas] = data_file[clas][i]\n",
    "            if type(essay) != float:\n",
    "                tmp.extend([w for w in essay.split()])\n",
    "                for w in tmp:\n",
    "                    splt = w.split(\"'\")\n",
    "                    for s in splt:\n",
    "                        if not s.isdigit():\n",
    "                            tmp_list.append(porter.stem(s))\n",
    "                for j in range(len(tmp_list)-1):\n",
    "                    essay_bigram_list.append(\" \".join((tmp_list[j],tmp_list[j+1])))\n",
    "                for k in range(len(tmp_list)-2):\n",
    "                    essay_trigram_list.append(\" \".join((tmp_list[k],tmp_list[k+1],tmp_list[k+2])))\n",
    "                unigrams_list.append((tmp_list, classifier_dictionary))\n",
    "                bigrams_list.append((essay_bigram_list, classifier_dictionary))\n",
    "                trigrams_list.append((essay_trigram_list, classifier_dictionary))\n",
    "        essay_unigrams[es] = unigrams_list\n",
    "        essay_bigrams[es] = bigrams_list\n",
    "        essay_trigrams[es] = trigrams_list\n",
    "    return (essay_unigrams, essay_bigrams, essay_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = ngram_generator_wlabels(sys.argv[1])\n",
    "outfile = open(sys.argv[2], 'wb')\n",
    "pickle.dump(ngrams, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.2 freq_ngrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
